{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_scifi_novels()` function grabs BeautifulSoup representation of a page of books.\n",
    "\n",
    "- First Page URL: https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books?page=1\n",
    "- Second Page URL: https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books?page=2\n",
    "\n",
    "- Base URL: https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books\n",
    "- N Page Url (0 to 63): ?page=N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scifi_novels(page_num):\n",
    "    print(\"Fetching top level page \", str(page_num))\n",
    "    base_url = \"http://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books\"\n",
    "    page_url = base_url + \"?page=\" + str(page_num)\n",
    "    r = requests.get(page_url)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    # Returns BeautifulSoup object for the first \n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parse_book_urls_from_page()` parses the BeautifulSoup object and returns a list of links.\n",
    "\n",
    "Example for page 1:\n",
    "\n",
    "```\n",
    "[\n",
    "    \"https://www.goodreads.com/book/show/375802.Ender_s_Game\",\n",
    "    \"https://www.goodreads.com/book/show/234225.Dune\",\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of book URL's from a single page from \"Best Science Fiction & Fantasy Books\" list\n",
    "def parse_book_urls_from_page(current_page):\n",
    "    current_page_books = current_page.find_all('a', attrs={'class': 'bookTitle'})\n",
    "    \n",
    "    # Parse the Goodreads book URL's and append to this list:\n",
    "    current_page_book_links = []\n",
    "\n",
    "    for book_a in current_page_books:\n",
    "        link = book_a.get('href')\n",
    "        full_link = \"http://www.goodreads.com\" + link\n",
    "        current_page_book_links.append(full_link)\n",
    "    # Returns list of book URLS.\n",
    "    return(current_page_book_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `parse_genre_page()` is a helper function for `parse_books()`\n",
    "- `parse_books()` accepts the list of book URL's and grabs interesting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function, used within parse_books()\n",
    "def parse_genre_page(genre_url):\n",
    "    r = requests.get(genre_url)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    tagged_genres = soup.find_all('a', attrs={'class': 'mediumText actionLinkLite'})\n",
    "    genre_names = []\n",
    "    genre_urls = []\n",
    "    for genre_html in tagged_genres:\n",
    "        genre_name = genre_html.text\n",
    "        genre_names.append(genre_name)\n",
    "        genre_url = \"http://www.goodreads.com\" + genre_html['href']\n",
    "        genre_urls.append(genre_url)\n",
    "    genre_dict = {}\n",
    "    genre_dict[\"names\"] = genre_names\n",
    "    genre_dict[\"urls\"] = genre_urls\n",
    "    return(genre_dict)\n",
    "\n",
    "# To test a quick example using parse_genre_page(), uncomment the following code:\n",
    "# parse_genre_page(\"https://www.goodreads.com/work/shelves/2422333\")\n",
    "\n",
    "# Fetch all of the interesting info for a <list> of book URLs.\n",
    "def parse_books(book_urls):\n",
    "    counter = 0\n",
    "    num_books = len(book_urls)\n",
    "    all_books_info = []\n",
    "    time.sleep(2)\n",
    "    for b_url in book_urls:\n",
    "        final_book_info = {}\n",
    "        # How many more books left?\n",
    "        counter += 1\n",
    "        print(\"Fetching Book \", str(counter), \" of \", str(num_books))\n",
    "\n",
    "        # Make request to the book page and parse using BeautifulSoup\n",
    "        r = requests.get(b_url)\n",
    "        soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "        # Start grabbing stuff!\n",
    "        # Book Title!\n",
    "        title_soup = soup.find('h1', attrs={'id': 'bookTitle'})\n",
    "        book_title = title_soup.text.replace(\"\\n\", \"\").replace(\"   \", \"\")\n",
    "\n",
    "        # Number of total ratings (0-5 stars)! Strip new-line characters + extraneous whitespace\n",
    "        num_ratings = soup.find('span', attrs={'class': \"votes value-title\"}).text.strip()\n",
    "        # Convert '100,000' to 100000\n",
    "        fmtd_num_ratings = num_ratings.replace(',', '')\n",
    "\n",
    "        # Tagged Genres\n",
    "        top_genres = {}\n",
    "        tagged_genre_html = soup.find_all('a', attrs={'class': 'bookPageGenreLink__seeMoreLink'})\n",
    "        if len(tagged_genre_html) > 0:\n",
    "            tagged_genre_page_url = \"http://www.goodreads.com\" + tagged_genre_html[0].get('href')\n",
    "            top_genres = parse_genre_page(tagged_genre_page_url)\n",
    "\n",
    "        # Reviews\n",
    "        # Number of total reviews. Strip new-line characters + extraneous whitespace\n",
    "        num_reviews = soup.find('span', attrs={'class': \"count value-title\"}).text.strip()\n",
    "        # Convert '100,000' to 100000\n",
    "        fmtd_num_reviews = num_reviews.replace(',', '')\n",
    "\n",
    "        # Reviews from the first page!\n",
    "        book_reviews = []\n",
    "        all_spans = soup.find('div', attrs={'id': 'bookReviews'}).find_all('span')\n",
    "\n",
    "        for sp in all_spans:\n",
    "            sp_id = sp.get('id')\n",
    "            if sp_id and sp_id.startswith('freeText'):\n",
    "                book_reviews.append(sp.text)\n",
    "        ###########\n",
    "                \n",
    "        # Plot\n",
    "        s = soup.find(\"div\", {\"id\": \"descriptionContainer\"})\n",
    "        \n",
    "        plot = \"NaN\"\n",
    "        if len(s.find_all(\"span\")) == 1:\n",
    "            plot = s.find_all(\"span\")[0].text\n",
    "        if len(s.find_all(\"span\")) == 2:\n",
    "            plot = s.find_all(\"span\")[1].text\n",
    "\n",
    "        # Author\n",
    "        author = \"NaN\"\n",
    "        author_block_one = soup.find(\"div\", {\"id\": \"aboutAuthor\"})\n",
    "        author_block_two = soup.find(\"a\", class_=\"authorName\")\n",
    "        \n",
    "        if author_block_one:\n",
    "            author = author_block_one.find(\"div\").text.lstrip(\"About\").lstrip()\n",
    "        if author_block_two:\n",
    "            author = author_block_two.text\n",
    "\n",
    "        # Published\n",
    "        publisher_block = soup.find_all(\"div\", class_=\"row\")\n",
    "\n",
    "        publisher_text = \"NaN\"\n",
    "        if len(publisher_block) > 1:\n",
    "            publisher_text = publisher_block[1].text.replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "        elif len(publisher_block) == 1:\n",
    "            publisher_text = publisher_block[0].text.replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "        \n",
    "        book_info = {}\n",
    "        book_info[\"url\"] = b_url\n",
    "        book_info[\"plot\"] = plot\n",
    "        book_info[\"author\"] = author\n",
    "        book_info[\"published\"] = publisher_text\n",
    "        \n",
    "        ###########\n",
    "        book_info[\"ratings\"] = {}\n",
    "        # Add number of user ratings (0-5 stars)\n",
    "        book_info[\"ratings\"][\"number\"] = fmtd_num_ratings\n",
    "        book_info[\"reviews\"] = {}\n",
    "        # Add number of reviews\n",
    "        book_info[\"reviews\"][\"number\"] = fmtd_num_reviews\n",
    "        # Add list of reviews\n",
    "        book_info[\"reviews\"][\"list_of_reviews\"] = book_reviews\n",
    "        # Add genres\n",
    "        book_info[\"genres\"] = top_genres\n",
    "        \n",
    "        ##########\n",
    "        book_info[\"url\"] = b_url\n",
    "        book_info[\"plot\"] = plot\n",
    "        book_info[\"author\"] = author\n",
    "        book_info[\"published\"] = publisher_text\n",
    "        ##########\n",
    "\n",
    "        # Assign nested dictionary containing the book's info to the final dict to return.\n",
    "        final_book_info[book_title] = book_info\n",
    "        \n",
    "        # Add to the final dictionary to be converted to JSON and dumped out to a file.\n",
    "        all_books_info.append(final_book_info)\n",
    "        \n",
    "    return(all_books_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glue Code That Downloads All Data By Calling Functions Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching top level page  62\n",
      "Fetching Book  1  of  100\n",
      "3\n",
      "Fetching Book  2  of  100\n",
      "3\n",
      "Fetching Book  3  of  100\n",
      "3\n",
      "Fetching Book  4  of  100\n",
      "3\n",
      "Fetching Book  5  of  100\n",
      "3\n",
      "Fetching Book  6  of  100\n",
      "2\n",
      "Fetching Book  7  of  100\n",
      "3\n",
      "Fetching Book  8  of  100\n",
      "3\n",
      "Fetching Book  9  of  100\n",
      "3\n",
      "Fetching Book  10  of  100\n",
      "3\n",
      "Fetching Book  11  of  100\n",
      "3\n",
      "Fetching Book  12  of  100\n",
      "2\n",
      "Fetching Book  13  of  100\n",
      "3\n",
      "Fetching Book  14  of  100\n",
      "3\n",
      "Fetching Book  15  of  100\n",
      "2\n",
      "Fetching Book  16  of  100\n",
      "3\n",
      "Fetching Book  17  of  100\n",
      "3\n",
      "Fetching Book  18  of  100\n",
      "3\n",
      "Fetching Book  19  of  100\n",
      "3\n",
      "Fetching Book  20  of  100\n",
      "3\n",
      "Fetching Book  21  of  100\n",
      "3\n",
      "Fetching Book  22  of  100\n",
      "3\n",
      "Fetching Book  23  of  100\n",
      "3\n",
      "Fetching Book  24  of  100\n",
      "2\n",
      "Fetching Book  25  of  100\n",
      "3\n",
      "Fetching Book  26  of  100\n",
      "3\n",
      "Fetching Book  27  of  100\n",
      "2\n",
      "Fetching Book  28  of  100\n",
      "3\n",
      "Fetching Book  29  of  100\n",
      "3\n",
      "Fetching Book  30  of  100\n",
      "3\n",
      "Fetching Book  31  of  100\n",
      "3\n",
      "Fetching Book  32  of  100\n",
      "3\n",
      "Fetching Book  33  of  100\n",
      "3\n",
      "Fetching Book  34  of  100\n",
      "3\n",
      "Fetching Book  35  of  100\n",
      "3\n",
      "Fetching Book  36  of  100\n",
      "3\n",
      "Fetching Book  37  of  100\n",
      "3\n",
      "Fetching Book  38  of  100\n",
      "3\n",
      "Fetching Book  39  of  100\n",
      "3\n",
      "Fetching Book  40  of  100\n",
      "3\n",
      "Fetching Book  41  of  100\n",
      "3\n",
      "Fetching Book  42  of  100\n",
      "3\n",
      "Fetching Book  43  of  100\n",
      "3\n",
      "Fetching Book  44  of  100\n",
      "3\n",
      "Fetching Book  45  of  100\n",
      "2\n",
      "Fetching Book  46  of  100\n",
      "3\n",
      "Fetching Book  47  of  100\n",
      "3\n",
      "Fetching Book  48  of  100\n",
      "3\n",
      "Fetching Book  49  of  100\n",
      "3\n",
      "Fetching Book  50  of  100\n",
      "3\n",
      "Fetching Book  51  of  100\n",
      "3\n",
      "Fetching Book  52  of  100\n",
      "3\n",
      "Fetching Book  53  of  100\n",
      "3\n",
      "Fetching Book  54  of  100\n",
      "3\n",
      "Fetching Book  55  of  100\n",
      "3\n",
      "Fetching Book  56  of  100\n",
      "3\n",
      "Fetching Book  57  of  100\n",
      "3\n",
      "Fetching Book  58  of  100\n",
      "3\n",
      "Fetching Book  59  of  100\n",
      "2\n",
      "Fetching Book  60  of  100\n",
      "3\n",
      "Fetching Book  61  of  100\n",
      "3\n",
      "Fetching Book  62  of  100\n",
      "3\n",
      "Fetching Book  63  of  100\n",
      "2\n",
      "Fetching Book  64  of  100\n",
      "3\n",
      "Fetching Book  65  of  100\n",
      "3\n",
      "Fetching Book  66  of  100\n",
      "3\n",
      "Fetching Book  67  of  100\n",
      "3\n",
      "Fetching Book  68  of  100\n",
      "3\n",
      "Fetching Book  69  of  100\n",
      "3\n",
      "Fetching Book  70  of  100\n",
      "3\n",
      "Fetching Book  71  of  100\n",
      "3\n",
      "Fetching Book  72  of  100\n",
      "3\n",
      "Fetching Book  73  of  100\n",
      "3\n",
      "Fetching Book  74  of  100\n",
      "3\n",
      "Fetching Book  75  of  100\n",
      "3\n",
      "Fetching Book  76  of  100\n",
      "3\n",
      "Fetching Book  77  of  100\n",
      "3\n",
      "Fetching Book  78  of  100\n",
      "3\n",
      "Fetching Book  79  of  100\n",
      "2\n",
      "Fetching Book  80  of  100\n",
      "3\n",
      "Fetching Book  81  of  100\n",
      "3\n",
      "Fetching Book  82  of  100\n",
      "3\n",
      "Fetching Book  83  of  100\n",
      "3\n",
      "Fetching Book  84  of  100\n",
      "3\n",
      "Fetching Book  85  of  100\n",
      "3\n",
      "Fetching Book  86  of  100\n",
      "3\n",
      "Fetching Book  87  of  100\n",
      "3\n",
      "Fetching Book  88  of  100\n",
      "2\n",
      "Fetching Book  89  of  100\n",
      "3\n",
      "Fetching Book  90  of  100\n",
      "3\n",
      "Fetching Book  91  of  100\n",
      "3\n",
      "Fetching Book  92  of  100\n",
      "3\n",
      "Fetching Book  93  of  100\n",
      "3\n",
      "Fetching Book  94  of  100\n",
      "3\n",
      "Fetching Book  95  of  100\n",
      "3\n",
      "Fetching Book  96  of  100\n",
      "3\n",
      "Fetching Book  97  of  100\n",
      "3\n",
      "Fetching Book  98  of  100\n",
      "3\n",
      "Fetching Book  99  of  100\n",
      "3\n",
      "Fetching Book  100  of  100\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,63):\n",
    "    page_soup = fetch_scifi_novels(i)\n",
    "    page_links = parse_book_urls_from_page(page_soup)\n",
    "    final_results = parse_books(page_links)\n",
    "    filename = \"page_\" + str(i) + \".json\"\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(final_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Counts Dictionary\n",
    "\n",
    "Read in downloaded data and build dictionary of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "counts = {\n",
    "    \"url\": 0,\n",
    "    \"plot\": 0,\n",
    "    \"author\": 0,\n",
    "    \"published\": 0,\n",
    "    \"ratings\": 0,\n",
    "    \"reviews\": 0,\n",
    "    \"genres\": 0\n",
    "}\n",
    "\n",
    "for i in range(0, 64):\n",
    "    data = json.load(open('page_' + str(i) + '.json'))\n",
    "    for book in data:\n",
    "        book_key = list(book.keys())[0]\n",
    "        book_key_list = book[book_key].keys()\n",
    "        for deep_key in book_key_list:\n",
    "            if book[book_key][deep_key] != \"NaN\":\n",
    "                counts[deep_key] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
